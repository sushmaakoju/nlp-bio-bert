# -*- coding: utf-8 -*-
"""hw3-hmm-viterbi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T-bEM4xUlfsoWjVmGYnn699D9hs2zTfB

### Named Entity Recognition over medical journal corpus

##### Homework 3, Fall, 2021
##### Prof. James H. Martin
###### author: Sushma Akoju

Notebook to train/fine-tune a BioBERT model to perform named entity recognition (NER). 

Required features:
  - Sentence id
  - Word
  - Tag

### This notebook includes
- Viterbi Dynamic programming approach with Trellis and back pointer approach.
- HMM aproach with Start, Transition, Emission probabilities for observations using Bigram approach. This is adaptable for trigram approach, which is not explored in this notebook in the interest of time as well as scope of homework
- Replacing RARE words with a RARE keyword to account for out-of-vocabulary and/or rare words in Test dataset.

### About the Dataset split;
- the dataset split is 80/20

#### References
- [Sequence Labeling for Parts of Speech and Named Entities:  ](https://web.stanford.edu/~jurafsky/slp3/8.pdf)
- [Hidden Markov Models:  ](https://web.stanford.edu/~jurafsky/slp3/A.pdf)
- [Viterbi algorithm: ](https://en.wikipedia.org/wiki/Viterbi_algorithm#Pseudocode)

#### Analysis:
- From above results, it is not recommended to continue using HMM and Viterbi approach for Named Entity Recognition, since the very pitfall of HMM model is that it is not flexible enough to unknown words as well as any new vocabulary words. It is possible to consider the previous word is a B tag, then next tag as an O tag, then the likelihood of having an I tag or O tag more than a B tag. However this consideration is not sufficient to generalize and improve HMM with Viterbi. 
- We could use additional features such as the first letter is a capital letter, all letters are capital letters, the previous word is a hyphen, or next word is a number, previous word + next word is an alphanumeric, all of which can act as better features for working with HMM. The idea is frequencies for each of these new features and their likelihood of being assigned a tag under a 5-gram approach, may work and seems like a reasonable approach to explore, but will be limiting to this approach alone.
"""

import numpy as np
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/Colab Notebooks/nlp-hw3'
!pwd

"""Preprocess the Data and analyze. 

"""

all_lines = {}
ent_tags = {}
tokens = []
with open("S21-gene-train.txt", "r", encoding="utf8", newline="\n") as file:
  lines = file.readlines()

new_line_counter = 0
all_raw_lines = []
sentences = []
sentence = []
word_tags = {}
this_sentence_tag_pairs = []
for i,line in enumerate(lines):
  if line != "\n":
    this_line = line.split("\t")
    sentence.append(this_line[1])
    #print(this_line)
    ent_tags[this_line[1].strip()] = this_line[2].strip()
    this_pair = this_line[1].strip(), this_line[2].strip()
    this_sentence_tag_pairs.append(this_pair)
    tokens.append(this_line[1].strip())
    if this_line[1].strip() not in word_tags.keys():
      word_tags[this_line[1].strip()] = {"O": 0, "I":0, "B":0 }
    word_tags[this_line[1].strip()][this_line[2].strip()] += 1
    all_raw_lines.append({"Sentence #":new_line_counter,"Line":int(this_line[0].strip()),"Word":this_line[1].strip(), "Tag":this_line[2].strip()})
  else:
    new_line_counter += 1
    all_lines[i] = this_sentence_tag_pairs
    this_sentence_tag_pairs = []
    sentences.append(" ".join(sentence))
    sentence = []

all_lines

"""#### About the data
- Total number of sentences: 13795
- Total number of words/tokens in dataset: 308229
- Max number of words in a sentence: ~102
- Vocabulary size: 27282
- Total number of B tags: 13304
- Total number of I tags: 19527
- Total number of O tags: 276009
- The most common top 10 words are: “.”, “the”, “of”, “-”, (',','and','in','a','(', 'to'.
- The least common top 10 words are: 'K713','hypercholesterolemic','lutein','P69','conference','Talk','Tele','cruciform','TE105'   
"""

train, test = train_test_split(list(all_lines.values()), test_size=0.2)
print(len(train), len(test)), train[2]

word_tag_pairs = [this_tuple for sent in train for this_tuple in sent]
all_words = [this_word_tag[0] for this_word_tag in word_tag_pairs]
all_tags = [this_word_tag[1] for this_word_tag in word_tag_pairs]
all_tag_string = ''.join(all_tags)
vocab = list(set(all_words))
unique_tags = set(all_tags)
tag_counter = Counter(all_tags)
word_tag_pair_counts = Counter(word_tag_pairs)

data = pd.DataFrame.from_records(all_raw_lines, index=range(1,len(all_raw_lines)+1) )
sent_word_count = data.groupby(['Sentence #']).count()['Word']
df = pd.DataFrame.from_dict(word_tags)
df_wt = pd.DataFrame.from_dict(word_tags).T
most_common = Counter(" ".join(data["Word"]).split()).most_common(10)
least_common = Counter(" ".join(data["Word"]).split()).most_common()[:-100-1:-1] 
data.head(5)

word_tag_pairs

print(len(word_tags.keys()), len(word_tags.values()))

print("Summary of this Dataset:")
print("Total number of sentences: %d"%(len(all_lines)))
print("Total number of words/tokens in dataset: %d" %len(tokens))
print("Max number of words in a sentence: %d"%(max(sent_word_count)))
print("Vocabulary size: %d"% (len(vocab)))
print("Total number of B tags:", tag_counter['B'])
print("Total number of I tags:", tag_counter['I'])
print("Total number of O tags:", tag_counter['O'])
print("The most common top 10 words are: ",most_common)
print("The least common top 10 words are: ",least_common)

"""Function to find transition probabilities a_ij : p(tag2|tag1)
##### Steps
  - Input: tag2, tag1 and all train_word_tag_pairs
  - Get all tags for the train_word_tag_pairs
  - count tag1 = count of tag1 in train_word_tag_pairs
  - sequence_counts = 0
  - for all word_tag pairs:
  - - for all tag1, tag2 sequence
    - sequence_counts += 1
  - return sequence_counts, count_tag1
#### Create Transition probabilities matrix for sequences

###### Steps:
  - Create a tag matrix tag_count * tag_count
  - Fill in each tag matrix with transition probability for the two tags in the

#### Create Emission Probabilities
###### Steps
  - Get count of tags that match the tag in word_tag_pairs
  - Get all word_tag_pairs that match the tag
  - Get count of words that match the word in word_tag_pairs that match the tag
  - return word_count_for_matched_tag, tag_count
"""

def get_emission_prob(tag, word, word_tag_pair_counts, tag_counter:Counter):
  #print(tag, word, word_tag_pair_counts)
  tag_count = tag_counter[tag]
  word_count_for_this_tag = word_tag_pair_counts[(word, tag)]
  return (word_count_for_this_tag, tag_count)

num, denom = get_emission_prob("B", "VirD2",word_tag_pair_counts, tag_counter)
num/denom

def get_transition_prob( tag2, tag1, all_tags_string, tag_counter):
  count1 = tag_counter[tag1]
  sequence_count = all_tag_string.count(tag1+tag2)
  #print(tag1+tag2, sequence_count, all_tag_string)
  return sequence_count, count1

def create_tag_transition_matrix(unique_tags:list,all_tags_string:str, tag_coutner:Counter):
  tag_matrix = np.zeros((len(unique_tags), len(unique_tags)), dtype="float32")
  for i,tag1 in enumerate(list(unique_tags)):
    for j,tag2 in enumerate(list(unique_tags)):
      seq_count, count1 = get_transition_prob( tag2, tag1, all_tags_string, tag_counter)
      tag_matrix[i, j] = seq_count/count1
  return tag_matrix

bigram_trans_prob_matrix = create_tag_transition_matrix(unique_tags, all_tag_string, tag_counter)
utags = list(unique_tags)
transition_df = pd.DataFrame(bigram_trans_prob_matrix, columns = utags, index=utags)
transition_df

"""Impement Viterbi with numpy and DP table (Trellis)
#### Viterbi Algorithm
- Inputs: Transition probabilities, emission probabilities, start probabilities, result_probabilities
###### Inputs/Variables
  - K is number of unique tags K = 3 (B,I,O)
  - Bigram case: number of states are BI, BB, BO, IO, II, IB, OI, OO, OB
  - Trigram case: number of states are 
    - BIB IIO OBO IBO BOO OBB BBO IOO IOI 
    - OOO IBI OBI IOB BBI OOB BII BOI IBB 
    - BOB BBB IIB OIB BIO OII OIO III OOI
  - Number of Tokens N = 31328 for this Dataset
  - Start probabalities are set of Initial probabilities and has size K * 1
  - Transition Probabilities Matrix T with a size K * K
  - Emission Matrix with a size N * K
  - End transition scores with a size of K * 1

###### Most Likely Hidden state Sequence
  - a hidden state sequence K * 1

###### Return value
  - Score of best sequence
  - Array of size N with integers (best sequence)

"""

K = len(unique_tags) #total unique tags : 3
N = len(word_tags.keys()) # total number of tokens : 32328
start_p = np.zeros((K)) # 3x1
trans_p = np.zeros((K,K)) # 3 x 3
end_scores =  np.zeros((K))
emission_p = np.zeros((N,K)) # 31328 x 3
tag_order = ['B', 'O', 'I']
tot_tags = len(all_tags)

tag_counter, utags, tag_order,tot_tags,

start_p.shape

count = 0
start_tags = []
for sent in train:
  tuples = [t[1] for t in sent[:1]]
  tag_seq = "".join(tuples)
  start_tags.append(tag_seq)
start_tag_counter = Counter(start_tags)
start_tag_counter['I'] = 0
tot_start_tags = sum(start_tag_counter.values())
start_tag_counter

for i,tag in enumerate(utags):
    start_p[i] = np.round(start_tag_counter[tag]/tot_tags, 3)

trans_p = np.asarray(bigram_trans_prob_matrix)
print(transition_df.head())

for i,word in enumerate(vocab):
  for j,tag in enumerate(utags):
    word_count_for_this_tag, tag_count = get_emission_prob(tag,word, word_tag_pair_counts, tag_counter)
    if tag_count != 0:
      emission_p[i][j] = word_count_for_this_tag / tag_count
    else:
      emission_p[i][j] = 0.0

e =  (emission_p - emission_p.min()) / (np.ptp(emission_p))
np.linalg.norm(emission_p/np.linalg.norm(emission_p)), e

for i, tag in enumerate(utags):
  num, denom = get_emission_prob(tag, ".", word_tag_pair_counts, tag_counter)
  end_scores[i] = num/denom

end_scores

from past.builtins import xrange
def viterbi_dp(start_p, trans_p, emission_p, end_scores):
  pred = []
  trellis = np.array([[0.0 for j in range(N)] for i in range(K)])
  backpointer = np.array([[-1 for j in range(N)] for i in range(K)])

  for i in xrange(N):
      if i == 0:
          trellis[:,0] = np.add(start_p, end_scores)
          print(trellis[:,0])
      else:
          for j in xrange(K):
              trellis[j,i] = np.max(np.add(trans_p[:,j], trellis[:,i-1] )) + emission_p[i,j]
              backpointer[j,i] = np.argmax(np.add(trans_p[:,j], trellis[:,i-1]))
  
  final_score = np.max(np.add(trellis[:, i], end_scores ))
  index = np.argmax(np.add(trellis[:, i], end_scores ))
  print(trellis)
  while index != -1:
      pred.append(index)
      index = argmax_trellis[index, i]
      i -= 1
  
  pred.reverse()
  return (final_score, pred)

score, pred = viterbi_dp(start_p, trans_p, emission_p, end_scores)

score

sum(pred) == len(word_tags.keys())

preds = np.asarray(pred)
preds[preds==1],preds[preds==2], utags, len(test),len(preds)

len(sentences[:len(train)]) ==len(train)

ngrams = None
def get_ngrams(train, n):
  for sent in train:
    #print(sent)
    word_boundary = (n-1) * [(None, "*")]
    word_boundary.extend(sent)
    word_boundary.append((None, "STOP"))
    ngrams = (tuple(word_boundary[i:i+n]) for i in xrange(len(word_boundary)-n+1))
    #print(ngrams)
    for ngram in ngrams:
      yield ngram

from collections import defaultdict

def train_hmm(train, n):
  emission_counts = defaultdict(int)
  ngram_counts = [defaultdict(int) for i in xrange(n)]
  for ngram in get_ngrams(train, n):
      
      assert len(ngram) == n, "n = %i, expected %i" %(len(ngram), n)
      #sent = [[t] for t in ngram]
      tags = tuple([tag for word, tag in ngram])

      for i in xrange(2, n+1):
        ngram_counts[i-1][tags[-i:]] += 1

      if ngram[-1][0] is not None:
        ngram_counts[0][tags[-1:]] += 1
        emission_counts[ngram[-1]] += 1
      
      if ngram[-(n-1)][0] is None:
        ngram_counts[n-2][tuple((n-1) * ["*"])] += 1
      
  return ngram_counts, emission_counts

bigram_counts, emit_counts = train_hmm(train, 2)
unigram_counts, uemit_counts = train_hmm(train, 1)

unigram_counts, tag_counter,bigram_counts, len(word_tag_pair_counts), len(emit_counts), len(uemit_counts)

word_tag_pair_counts[('cancer', 'O')], emit_counts[('cancer', 'O')], uemit_counts[('cancer', 'O')],unigram_counts

tag_counter,word_tag_pair_counts

d = defaultdict(float)
d_rare = []
for w,c in Counter(all_words).items():
  if c < 5:
    d_rare.append(word)

Counter(all_words).most_common()[::-1]

RARE_KEYWORD = "_RARE_"
word_tag_pair_counts_r = word_tag_pair_counts.copy()
for wt, count in word_tag_pair_counts.items():
  word, tag = wt
  if word in d_rare:
    word_tag_pair_counts_r[(RARE_KEYWORD, tag)] = count
  #print(tag_counter[tag], tag, word, count)

for wt, count in word_tag_pair_counts_r.items():
  word, tag = wt
  p = count*  1.0 /tag_counter[tag]
  d[(word, tag)] = p
d

"""### Approch for RARE Words
For each word that has a frequency of less than 5 i.e. the word occurs less than 5 times, the word is replaced with *_RARE_* . Thus, any word that is not in the vocabulary, the zero frequencies for new word appearing in the Test dataset is almost acocunted for.
"""

def get_rare_word_counts(d):
  max_e = 0.0
  max_label = ""
  if (RARE_KEYWORD,'I') in d:
    if d[(RARE_KEYWORD, 'I')] > max_e:
      max_e = d[(RARE_KEYWORD,'I')]
      max_label = 'I'
  if (RARE_KEYWORD,'B') in d:
    if d[(RARE_KEYWORD, 'B')] > max_e:
      max_e = d[(RARE_KEYWORD,'B')]
      max_label = 'B'
  if (RARE_KEYWORD,'O') in d:
    if d[(RARE_KEYWORD, 'O')] > max_e:
      max_e = d[(RARE_KEYWORD,'O')]
      max_label = 'O'
  return max_e, max_label

import math
preds_new = []
full_preds = []
max_e_rare, max_label_rare = get_rare_word_counts(d)
for sent in test:
  this_sent = []
  counter = 1
  for wt in sent:

    w,_ = wt
    max_e = 0.0
    max_label = ""
    if (w,'I') in d:
      if d[(w, 'I')] > max_e:
        max_e = d[(w,'I')]
        max_label = 'I'
    if (w,'B') in d:
      if d[(w, 'B')] > max_e:
        max_e = d[(w,'B')]
        max_label = 'B'
    if (w,'O') in d:
      if d[(w, 'O')] > max_e:
        max_e = d[(w,'O')]
        max_label = 'O'

    if max_e == 0.0 and max_label == "":
      max_e = max_e_rare
      max_label = max_label_rare
    
    #print(max_e, max_e_rare, w, d)
    full_preds.append((w, max_label, math.log(max_e, 2)))
    this_sent.append('\t'.join([str(counter), w, max_label])+'\n' )
    counter += 1
  preds_new.append(this_sent)

full_preds[:5], preds_new[:10]

len(preds_new) == len(test)

with open("preds_hmm.csv", 'w') as f:
  for s in preds_new:
    print("".join(s))
    f.write("".join(s))
f

"""#### The results and analysis
The results are not as satisfactory which further compelled to attempt other approaches towards the problem to NER task.
"""