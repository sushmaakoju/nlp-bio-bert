# -*- coding: utf-8 -*-
"""model_predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P3Sa6ljbKx-q8Vy_kxL7eho_nPMoUwWW

### Named Entity Recognition over biomedical journal corpus

##### Homework 3, Fall, 2021
##### Prof. James H. Martin
###### author: Sushma Akoju

#### Named Entity Recognition over medical journal corpus

#### Goal: 
The task in homework suggests to detect “Gene” entities which could appear in patterns such as “BII”, “BIO”, or even regex such as “BI[I]*[O]*”.

#### About this Notebook
This only for classwise evaluation purpose.
The notebook first finds start, end pairs for each tag of a word for each sentence. Then calcuates Confusion matrix and finally the precision, recall and f1-scores for each of tag predicted by a given model predictions.
This is only conducted for detailed analysis of predicted entities.
"""

#Import all required libraries
import spacy
import random
import time
import numpy as np
from spacy.util import minibatch, compounding
import sys
from spacy import displacy
from itertools import chain
import matplotlib.pyplot as plt 
from matplotlib.ticker import MaxNLocator

from google.colab import drive
drive.mount('/content/drive')

import os
os.path.exists('/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/train.tsv')

def load_data2spacy_hmm(file_path):
      ent_labels = []
      ents = []
      file = open(file_path, 'r', encoding = "utf8", newline = "\n")
      training_data, entities, sentence = [], [], []
      unique_tags = []
      curr_annotattion = None
      start = 0
      end = 0
      lines = file.readlines()
      print(len(lines))
      for line in lines:
        #print(line, len(line.strip().split("\t")))
        line = [l.strip() for l in line.strip().split("\t")[1:]]
        if len(line) > 1:
            tag = line[1].strip()
            #maybe an human annotation typo?
            if tag == '1':
              tag = 'I'
            else:
              tag = line[1].strip()

            word = line[0].strip()
            sentence.append(word)
            start = end
            end += (len(word) + 1)

            if tag == 'I':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            if tag == 'B':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            if tag == 'O':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            
            if tag not in unique_tags:
                unique_tags.append(tag)
    
        if len(line) <= 1:
              if len(entities) > 0:
                  sentence = " ".join(sentence)
                  training_data.append([sentence, {'entities': entities}])
                  ent_labels.append(ents)
              #print(sentence, entities)
              end = 0
              start =0
              entities, sentence = [], []
              ents = []
    
      file.close()
      #print(training_data, unique_tags)
      print(ent_labels)
      return training_data,ent_labels, unique_tags

pred, ent_pred, test_tags= load_data2spacy_hmm("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/preds_hmm.csv")
test, ent_test, pred_tags = load_data2spacy_hmm("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/test_hmm.csv")

ent_pred

def load_data2spacy(file_path):
      file = open(file_path, 'r', encoding = "utf8", newline = "\n")
      training_data, entities, sentence = [], [], []
      unique_tags = []
      curr_annotattion = None
      start = 0
      end = 0
      lines = file.readlines()
      for line in lines:
        #print(line, line.strip().split("\t"))
        line = [l.strip() for l in line.strip().split("\t")[1:]]
        if len(line) > 1:
            tag = line[1].strip()
            #maybe an human annotation typo?
            if tag == '1':
              tag = 'I'
            else:
              tag = line[1].strip()

            word = line[0].strip()
            sentence.append(word)
            start = end
            end += (len(word) + 1)

            if tag == 'I':
                entities.append((start, end-1, tag))

            if tag == 'B':
                entities.append((start, end-1, tag))

            if tag == 'O':
                entities.append((start, end-1, tag))

            
            if tag not in unique_tags:
                unique_tags.append(tag)
    
        if len(line) <= 1:
              if len(entities) > 0:
                  sentence = " ".join(sentence)
                  training_data.append([sentence, {'entities': entities}])
              #print(sentence, entities)
              end = 0
              start =0
              entities, sentence = [], []
    
      file.close()
      print(training_data, unique_tags)
      return training_data, unique_tags

from typing import List, Dict, Sequence

class MultiLabelEvaluation:
    def __init__(self, true_sentence_tags: Sequence[Sequence[Dict]],
                 prediction_sentence_tags: Sequence[Sequence[Dict]]):
        self.true_sentence_tags =  true_sentence_tags
        self.prediction_sentence_tags = prediction_sentence_tags
        self.types = set(entity['type'] for sentence in true_sentence_tags for entity in sentence)
        self.confusion_matrices = {type:{'TP':0, 'TN':0, 'FP':0, 'FN':0} for type in self.types}
        self.scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}
           
    def cal_confusion_matrices(self) -> Dict[str, Dict]:
        for true_labels, pred_labels in zip(self.true_sentence_tags, self.prediction_sentence_tags):
            for true_label in true_labels: 
                entity_type = true_label['type']
                prediction_hit_count = 0 
                for pred_label in pred_labels:
                    if pred_label['type'] != entity_type:
                        continue
                    if pred_label['start_idx'] == true_label['start_idx'] and pred_label['end_idx'] == true_label['end_idx'] and pred_label['text'] == true_label['text']: # TP
                        self.confusion_matrices[entity_type]['TP'] += 1
                        prediction_hit_count += 1
                    elif ((pred_label['start_idx'] == true_label['start_idx']) or (pred_label['end_idx'] == true_label['end_idx'])) and pred_label['text'] != true_label['text']: # boundry error, count FN, FP
                        self.confusion_matrices[entity_type]['FP'] += 1
                        self.confusion_matrices[entity_type]['FN'] += 1
                        prediction_hit_count += 1
                if prediction_hit_count != 1: # FN, model cannot make a prediction for true_label
                    self.confusion_matrices[entity_type]['FN'] += 1
                prediction_hit_count = 0 # reset to default

    def cal_scores(self) -> Dict[str, Dict]:
        """Calculate precision, recall, f1."""
        confusion_matrices = self.confusion_matrices 
        scores = {type: {'p': 0, 'r': 0, 'f1': 0} for type in self.types}
        
        for entity_type, confusion_matrix in confusion_matrices.items():
            if confusion_matrix['TP'] == 0 and confusion_matrix['FP'] == 0:
                scores[entity_type]['p'] = 0
            else:
                scores[entity_type]['p'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FP'])

            if confusion_matrix['TP'] == 0 and confusion_matrix['FN'] == 0:
                scores[entity_type]['r'] = 0
            else:
                scores[entity_type]['r'] = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FN']) 

            if scores[entity_type]['p'] == 0 or scores[entity_type]['r'] == 0:
                scores[entity_type]['f1'] = 0
            else:
                scores[entity_type]['f1'] = 2*scores[entity_type]['p']*scores[entity_type]['r'] / (scores[entity_type]['p']+scores[entity_type]['r'])  
        self.scores = scores

    def print_confusion_matrices(self):
        for entity_type, matrix in self.confusion_matrices.items():
            print(f"{entity_type}: {matrix}")

    def print_scores(self):
        for entity_type, score in self.scores.items():
            print(f"{entity_type}: f1 {score['f1']:.4f}, precision {score['p']:.4f}, recall {score['r']:.4f}")

label_eval_obj = MultiLabelEvaluation(ent_test, ent_pred)
label_eval_obj.cal_confusion_matrices()
label_eval_obj.print_confusion_matrices()
label_eval_obj.cal_scores()
label_eval_obj.print_scores()

def load_data2spacy(file_path):
      ent_labels = []
      ents = []
      file = open(file_path, 'r', encoding = "utf8", newline = "\n")
      training_data, entities, sentence = [], [], []
      unique_tags = []
      curr_annotattion = None
      start = 0
      end = 0
      lines = file.readlines()
      counter = 0
      for line in lines:
        #print(line, line.strip().split("\t"))
        line = [l.strip() for l in line.strip().split("\t")]
        if len(line) > 1:
            tag = line[2].strip()
            #maybe an human annotation typo?
            if tag == '1':
              tag = 'I'
            else:
              tag = line[2].strip()

            word = line[1].strip()
            sentence.append(word)
            start = end
            end += (len(word) + 1)
            #print(word,start, end, tag)
            if tag == 'I':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            if tag == 'B':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            if tag == 'O':
                entities.append((start, end-1, tag))
                ents.append({'start_idx': start, 'end_idx': end-1, 'text': word, 'type': tag})

            if tag not in unique_tags:
                unique_tags.append(tag)
        #print(len(line))
        if counter+1 < len(lines):
            li = [l.strip() for l in lines[counter+1].strip().split("\t")][0]
            #print(li)
            if li == '1':
                #print(line, "len<1")
                if len(entities) > 0:
                    sentence = " ".join(sentence)
                    training_data.append([sentence, {'entities': entities}])
                    ent_labels.append(ents)
                #print(sentence, entities)
                end = 0
                start =0
                entities, sentence = [], []
                ents = []
        counter += 1
    
      file.close()
      #print(training_data, unique_tags)
      print(ent_labels)
      return ent_labels, unique_tags

pred, tags= load_data2spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/preds_ib.csv")
test, tags = load_data2spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/test.csv")

label_eval_obj = MultiLabelEvaluation(test, pred)
label_eval_obj.cal_confusion_matrices()
label_eval_obj.print_confusion_matrices()
label_eval_obj.cal_scores()
label_eval_obj.print_scores()

pred, tags= load_data2spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/preds_bio.csv")
#test, tags = load_data2spacy_spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/test.csv")

label_eval_obj = MultiLabelEvaluation(test, pred)
label_eval_obj.cal_confusion_matrices()
label_eval_obj.print_confusion_matrices()
label_eval_obj.cal_scores()
label_eval_obj.print_scores()

pred, tags= load_data2spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/preds_biobert.csv")

label_eval_obj = MultiLabelEvaluation(test, pred)
label_eval_obj.cal_confusion_matrices()
label_eval_obj.print_confusion_matrices()
label_eval_obj.cal_scores()
label_eval_obj.print_scores()

TEST_DATA, _ = load_data2spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/test.tsv")
print(len(TEST_DATA))

pred, tags= load_data_spacy("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hmm/hmm_pred.tsv")

print(len(test))
print(len(pred))

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3

def load_model(model_path):
    ''' Loads a pre-trained model for prediction on new test sentences
   
    model_path : directory of model saved by spacy.to_disk
    '''
    nlp = spacy.blank('en')
    if 'ner' not in nlp.pipe_names:
        ner = nlp.create_pipe('ner')
        nlp.add_pipe(ner)
    ner = nlp.from_disk(model_path)
    return ner

ner1 = load_model("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hw3_ner_spacy_GloVE")
ner2 = load_model("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/hw3_ner_spacy_GloVE_bio")

predictions_ib = []
predictions_bio = []

test_sentences = [x[0] for x in TEST_DATA] # extract the sentences from [sentence, entity]
for x in test_sentences:

    this_sentence = x.split()

    doc1 = ner1(x)
    doc2 = ner2(x)

    l1 = {}
    l2 = {}

    print("*****************This is for NER model using  I, B tags***************************")
    ################################### NER IB model #########################################3
    for ent in doc1.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)
        l1[ent.text] = ent.label_

    counter = 1
    for word in this_sentence:
      if word in list(l1.keys()):
        predictions_ib.append([counter, word, l1[word]])
      else:
        predictions_ib.append([counter, word, 'O'])
      counter +=1
    predictions_ib.append(["\n"])

    displacy.render(doc1, jupyter=True, style = "ent")

    #################################### ner IOB model ###########################
    print("*****************This is for NER model using  I, O, B tags***************************")
    for ent in doc2.ents:
        print(ent.text, ent.start_char, ent.end_char, ent.label_)
        l2[ent.text] = ent.label_
    
    counter = 1
    for word in this_sentence:
      if word in list(l2.keys()):
        predictions_bio.append([counter, word, l2[word]])
      counter +=1
    predictions_bio.append(["\n"])

    displacy.render(doc2, jupyter=True, style = "ent")

import pandas as pd
 df1 = pd.DataFrame.from_records(predictions_ib)
 df2 = pd.DataFrame.from_records(predictions_bio)

 df1.head()

d1 = df1.to_numpy().tolist()
len(d1)

new_df1 = []
for d in d1:
  if d[0] == '\n' and d[1] == None and d[2] == None:
    d[0].strip()
  elif d[0] == None and d[1] == None:
    d[0].strip()
  else:
    new_df1.append(d)

new_df1[:20]

df11 = pd.DataFrame.from_records(new_df1)
df11.head(30)

df2.head()

d2 = df2.to_numpy().tolist()
len(d1)
new_df2 = []
for d in d2:
  if d[0] == '\n' and d[1] == None and d[2] == None:
    d[0].strip()
  elif d[0] == None and d[1] == None:
    d[0].strip()
  else:
    new_df2.append(d)
new_df2[:30]

df22 = pd.DataFrame.from_records(new_df2)
df22.head(30)

df11.to_csv('preds_ib.csv', sep="\t", header=False, index = False)
df22.to_csv('preds_bio.csv', sep="\t", header=False, index = False)

df11.info()

df22.info()

dft = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/entity-extraction/hw3/test.tsv", sep="\t")
dft.head(30)

d3 = dft.to_numpy().tolist()
d3[20:80]

counter = 1
new_df3 = []
for d in d3:
  if d[1] == '\n' and type(d[2]) == float:
    d[1].strip()
    counter = 1
  else:
    d[0] = counter
    new_df3.append(d)
    counter += 1
new_df3[:30]

df33 = pd.DataFrame.from_records(new_df3)
df33.head(30)

df33.to_csv('test_data.csv', sep="\t", header=False, index = False)